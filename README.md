<div align="center">
  
# PoE-Evaluation: Product of Experts para Avalia√ß√£o de LLMs

**Framework completo para avalia√ß√£o multidimensional de modelos de linguagem em tarefas de gera√ß√£o de c√≥digo**

[![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Ollama](https://img.shields.io/badge/Ollama-000000?style=for-the-badge&logo=ollama&logoColor=white)](https://ollama.com/)
[![HumanEval](https://img.shields.io/badge/HumanEval-FF6B6B?style=for-the-badge&logo=openai&logoColor=white)](https://github.com/openai/human-eval)

</div>

---

## ‚ùáÔ∏è √çndice
- [Sobre o Projeto](#sobre-o-projeto)
- [Arquitetura](#arquitetura)
- [Pr√©-requisitos](#pre-requisitos)
- [Instala√ß√£o](#instalacao)
- [Pipeline Completo](#pipeline-completo)
- [M√©tricas do Product of Experts](#metricas-do-product-of-experts)
- [Interpreta√ß√£o dos Resultados](#interpretacao-dos-resultados)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Comandos √öteis](#comandos-uteis)
- [Refer√™ncias](#referencias)
- [Licen√ßa](#licenca)
- [Autores](#autores)
- [Cita√ß√£o Acad√™mica](#citacao-academica)

---

<a name="sobre-o-projeto"></a>
## ‚ùáÔ∏è Sobre o Projeto

O **PoE-Evaluation** √© um framework robusto para avalia√ß√£o multidimensional de Large Language Models (LLMs) em tarefas de gera√ß√£o de c√≥digo. Utilizando o benchmark **HumanEval** como base, o projeto implementa a metodologia **Product of Experts (PoE)** para combinar m√∫ltiplas m√©tricas de qualidade em um score unificado e interpret√°vel.

### Principais Caracter√≠sticas

- **üéØ Avalia√ß√£o Hol√≠stica**: Combina corre√ß√£o funcional, complexidade, estilo, efici√™ncia e concis√£o
- **üßÆ Product of Experts**: Algoritmo matem√°tico que pondera m√∫ltiplas m√©tricas especializadas
- **üê≥ Infraestrutura Containerizada**: Ambiente isolado e 100% reproduz√≠vel
- **üìä Visualiza√ß√£o Avan√ßada**: Gr√°ficos comparativos automatizados entre modelos
- **üîÑ Pipeline Automatizado**: Desde a gera√ß√£o at√© a an√°lise, tudo em poucos comandos
- **üåê Compat√≠vel com Ollama**: Suporta qualquer modelo dispon√≠vel na biblioteca oficial

### Use Cases

- Benchmark de modelos de c√≥digo (CodeLlama, StarCoder, DeepSeek-Coder, etc.)
- Pesquisa acad√™mica em avalia√ß√£o de LLMs
- Sele√ß√£o de modelos para ambientes de produ√ß√£o
- An√°lise de trade-offs entre tamanho e qualidade de modelos

---

<a name="arquitetura"></a>
## ‚ùáÔ∏è Arquitetura

```mermaid
graph TB
    subgraph "Container Ollama"
        A[Ollama Server<br/>:11434]
        A1[codegema:7b]
        A2[codellama:7b]
        A3[qwen2.5-coder:7b]
        A4[starcoder2:7b]
        A5[deepseek-coder:6.7b]
        A --> A1 & A2 & A3 & A4 & A5
    end
    
    subgraph "Container HumanEval Sandbox"
        B[generate_samples.py]
        C[enhanced_eval.py]
        D[overall_score.py]
    end
    
    subgraph "M√©tricas Especializadas"
        E1[HumanEval<br/>Corre√ß√£o]
        E2[Radon<br/>Complexidade]
        E3[Pylint<br/>Estilo]
        E4[Timer<br/>Efici√™ncia]
        E5[LOC<br/>Concis√£o]
    end
    
    subgraph "Armazenamento"
        F1[(samples/)]
        F2[(results/)]
        F3[(plots/)]
    end
    
    A -->|API REST| B
    B --> F1
    F1 --> C
    C --> E1 & E2 & E3 & E4 & E5
    E1 & E2 & E3 & E4 & E5 -->|PoE| C
    C --> F2
    F2 --> D
    D --> F3
    
    %% Estilos das caixas com texto preto
    style A fill:#4A90E2, color:#000
    style B fill:#7ED321, color:#000
    style C fill:#F5A623, color:#000
    style D fill:#BD10E0, color:#000
    style E1 fill:#50E3C2, color:#000
    style E2 fill:#50E3C2, color:#000
    style E3 fill:#50E3C2, color:#000
    style E4 fill:#50E3C2, color:#000
    style E5 fill:#50E3C2, color:#000
```

### Componentes

| Componente | Fun√ß√£o | Tecnologia |
|------------|--------|------------|
| **Ollama** | Hospeda e serve modelos LLM via API REST | Ollama Server |
| **HumanEval Sandbox** | Ambiente isolado para execu√ß√£o e testes | Docker + Python |
| **generate_samples.py** | Gera solu√ß√µes de c√≥digo para 164 problemas | Requests + HumanEval |
| **enhanced_eval.py** | Avalia 5 m√©tricas e aplica PoE | Radon + Pylint + HumanEval |
| **overall_score.py** | Gera visualiza√ß√µes comparativas | Matplotlib + Seaborn |

---

<a name="pre-requisitos"></a>
## ‚ùáÔ∏è Pr√©-requisitos

### Software Necess√°rio

- **Docker Desktop** 20.10+ ([Download](https://www.docker.com/products/docker-desktop/))
- **Docker Compose** 2.0+ (inclu√≠do no Docker Desktop)
- **Git** (para clonar o reposit√≥rio)

### Requisitos de Sistema

| Recurso | M√≠nimo | Recomendado |
|---------|--------|-------------|
| RAM | 8 GB | 16 GB+ |
| CPU | 4 cores | 8+ cores |
| Armazenamento | 15 GB | 30 GB+ |
| SO | Linux/macOS/Windows | Linux (melhor performance) |

---

<a name="instalacao"></a>
## ‚ùáÔ∏è Instala√ß√£o

### 1. Clone o Reposit√≥rio

```bash
git clone https://github.com/tcc-inatel-2025/PoE-Evaluation.git
cd PoE-Evaluation
```

### 2. Construa os Containers

```bash
docker compose build
```

### 3. Inicie os Servi√ßos

```bash
docker compose up -d
```

### 4. Verifique o Status

```bash
docker ps
```

---

<a name="pipeline-completo"></a>
## ‚ùáÔ∏è Pipeline Completo

### Vis√£o Geral do Fluxo

```mermaid
sequenceDiagram
    participant User
    participant Ollama
    participant Generator as generate_samples.py
    participant Evaluator as enhanced_eval.py
    participant Visualizer as overall_score.py
    
    User->>Ollama: 1. Baixar modelo
    User->>Generator: 2. Gerar samples
    Generator->>Ollama: Requisitar solu√ß√µes
    Ollama-->>Generator: C√≥digo gerado
    Generator->>Generator: Salvar samples.jsonl
    
    User->>Evaluator: 3. Avaliar com PoE
    Evaluator->>Evaluator: Testes HumanEval
    Evaluator->>Evaluator: Radon (Complexidade)
    Evaluator->>Evaluator: Pylint (Estilo)
    Evaluator->>Evaluator: Timer (Efici√™ncia)
    Evaluator->>Evaluator: Calcular PoE Score
    Evaluator->>Evaluator: Salvar results.jsonl
    
    User->>Visualizer: 4. Gerar gr√°ficos
    Visualizer->>Visualizer: Ler CSVs
    Visualizer->>Visualizer: Criar plots
    Visualizer-->>User: PNG files
```

---

### Passo 1.: Baixar Modelos no Ollama

Entre no container do Ollama:

```bash
docker exec -it ollama bash
```

Baixe os modelos desejados:

```bash
# Modelos m√©dios (1-7B par√¢metros)
ollama pull codegema:7b
ollama pull codellama:7b
ollama pull qwen2.5-coder:7b
ollama pull starcoder2:7b

# Modelos avan√ßados (> 7B par√¢metros)
ollama pull deepseek-coder:6.7b
```

**Dica**: Comece com modelos pequenos para testar o pipeline antes de avaliar modelos maiores.

Saia do container:

```bash
exit
```

---

### Passo 2.: Gerar Amostras de C√≥digo

Entre no container do HumanEval:

```bash
docker exec -it humaneval_sandbox bash
cd /workspace
```

Execute o gerador:

```bash
python generate_samples.py
```

**Par√¢metros dispon√≠veis:**

| Par√¢metro | Descri√ß√£o | Padr√£o |
|-----------|-----------|--------|
| `--model` | Nome do modelo no Ollama | `smollm2:135m` |
| `--url` | URL do servidor Ollama | `http://ollama:11434` |
| `--num-samples` | Amostras por problema | `1` |
| `--output` | Arquivo de sa√≠da customizado | `samples/{model}_samples.jsonl` |

---

### Passo 3.: Avaliar com Product of Experts

Execute a avalia√ß√£o completa:

```bash
python enhanced_eval.py 
```

**O que acontece nesta etapa:**

1. **üß™ Corre√ß√£o Funcional**: Executa testes unit√°rios do HumanEval
2. **üìä Complexidade Ciclom√°tica**: Analisa com Radon 
3. **üé® Qualidade de Estilo**: Verifica com Pylint (score 0-10)
4. **‚ö° Efici√™ncia**: Mede tempo de execu√ß√£o
5. **üìè Concis√£o**: Conta linhas de c√≥digo (LOC)
6. **üßÆ Score PoE**: Calcula produto geom√©trico normalizado

**Arquivos gerados:**
- `results/deepseek-coder_6.7b_results.jsonl` ‚Üí Resultados detalhados
- `results/summary/deepseek-coder_6.7b_summary.csv` ‚Üí Resumo para an√°lise

---

### Passo 4.: Visualizar Resultados

Gere gr√°ficos comparativos:

```bash
python overall_score.py
```

**Gr√°ficos gerados:**

1. **`overall_score_distribution.png`**
   - Distribui√ß√£o dos scores PoE
   - Histograma + KDE
   
2. **`metrics_mean.png`**
   - M√©dia de cada m√©trica individual
   - Gr√°fico de barras horizontais

3. **`metrics_correlation.png`**
   - Heatmap de correla√ß√£o entre m√©tricas
   - Identifica depend√™ncias

4. **`model_comparison.png`**
   - Compara√ß√£o lado a lado de todos os modelos
   - Heatmap multidimensional

---

### Passo 5.: Exportar Resultados para o Host

No terminal do seu computador (fora dos containers):

```bash
# Exportar todos os resultados
docker cp humaneval_sandbox:/workspace/samples ./
docker cp humaneval_sandbox:/workspace/results ./
docker cp humaneval_sandbox:/workspace/plots ./
```

Agora voc√™ ter√° toda a estrutura de arquivos localmente!

---

<a name="metricas-do-product-of-experts"></a>
## ‚ùáÔ∏è M√©tricas do Product of Experts

### F√≥rmula Matem√°tica

O score PoE combina m√∫ltiplas m√©tricas usando m√©dia geom√©trica ponderada:

```
Score_PoE = Functional_Correctness √ó Quality_Score

onde:

Quality_Score = (CC^Œ± √ó Style^Œ≤ √ó Efficiency^Œ≥ √ó LOC^Œ¥)^(1/4)

Pesos:
Œ± = 1 (Complexidade Ciclom√°tica)
Œ≤ = 1 (Estilo/Pylint)
Œ≥ = 1 (Efici√™ncia)
Œ¥ = 1 (Lines of Code)
```

### M√©tricas Individuais

| M√©trica | Ferramenta | Escala | Descri√ß√£o | Peso |
|---------|-----------|--------|-----------|------|
| **Corre√ß√£o Funcional** | HumanEval | {0, 1} | Passa em todos os testes? | Bin√°rio (gate) |
| **Complexidade Ciclom√°tica** | Radon | [0, 1] | Simplicidade do c√≥digo | 25% |
| **Qualidade de Estilo** | Pylint | [0, 1] | Conformidade PEP 8 | 25% |
| **Efici√™ncia** | Timer | [0, 1] | Tempo de execu√ß√£o | 25% |
| **Concis√£o (LOC)** | Contador | [0, 1] | Menos linhas = melhor | 25% |

### Escalamento das M√©tricas

#### 1. Complexidade Ciclom√°tica (CC)

```python
def scale_cc(value, max_cc=10):
    """
    CC = 1-5   -> Simples (score alto)
    CC = 6-10  -> Moderado
    CC > 10    -> Complexo (score baixo)
    """
    raw_scaled = min(value / max_cc, 1.0)
    return 1.0 - raw_scaled  # Inverte: menos complexidade = melhor
```

**Interpreta√ß√£o:**
- CC = 1: `score = 0.90` (excelente)
- CC = 5: `score = 0.50` (aceit√°vel)
- CC = 15: `score = 0.00` (muito complexo)

#### 2. Estilo (Pylint)

```python
def scale_lint(value, max_score=10):
    """
    Pylint retorna score 0-10
    """
    return min(value / max_score, 1.0)
```

**Interpreta√ß√£o:**
- Pylint 9-10: `score = 0.90-1.00` (excelente)
- Pylint 7-8: `score = 0.70-0.80` (bom)
- Pylint < 5: `score < 0.50` (precisa melhorar)

#### 3. Efici√™ncia

```python
def scale_efficiency(value, max_time=1.0):
    """
    Tempo em segundos
    """
    raw_scaled = min(value / max_time, 1.0)
    return 1.0 - raw_scaled  # Menos tempo = melhor
```

**Interpreta√ß√£o:**
- 0.1s: `score = 0.90` (muito r√°pido)
- 0.5s: `score = 0.50` (m√©dio)
- > 1.0s: `score = 0.00` (lento)

#### 4. Concis√£o (LOC)

```python
def scale_loc(loc, ref=50.0):
    """
    N√∫mero de linhas
    """
    return max(0.0, 1.0 - min(loc / ref, 1.0))
```

**Interpreta√ß√£o:**
- 10 linhas: `score = 0.80` (conciso)
- 25 linhas: `score = 0.50` (m√©dio)
- 50+ linhas: `score = 0.00` (verboso)

---

<a name="interpretacao-dos-resultados"></a>
## ‚ùáÔ∏è Interpreta√ß√£o dos Resultados

### Classifica√ß√£o de Scores

| Score PoE | Badge | Interpreta√ß√£o | A√ß√£o Recomendada |
|-----------|-------|---------------|-------------------|
| 0.90 - 1.00 | üèÜ **Excelente** | C√≥digo production-ready | Deploy com confian√ßa |
| 0.75 - 0.89 | ‚úÖ **Bom** | Qualidade aceit√°vel | Pequenos ajustes opcionais |
| 0.60 - 0.74 | ‚ö†Ô∏è **Regular** | Funcional mas melhor√°vel | Refatora√ß√£o recomendada |
| 0.40 - 0.59 | ‚ùå **Ruim** | Problemas significativos | Revis√£o necess√°ria |
| 0.00 - 0.39 | üö´ **Cr√≠tico** | N√£o usar em produ√ß√£o | Reescrever |

---

<a name="estrutura-do-projeto"></a>
## ‚ùáÔ∏è Estrutura do Projeto

### Arquivos-Chave

| Arquivo | Descri√ß√£o | Formato |
|---------|-----------|---------|
| `*_samples.jsonl` | Solu√ß√µes geradas pelo LLM | `{"task_id": "...", "completion": "..."}` |
| `*_results.jsonl` | Resultados detalhados + m√©tricas | `{"task_id": "...", "passed": true, ...}` |
| `*_summary.csv` | Resumo tabelado para an√°lise | Pandas DataFrame |
| `*.png` | Gr√°ficos comparativos | Imagem (300 DPI) |

---

<a name="comandos-uteis"></a>
## ‚ùáÔ∏è Comandos √öteis

### Gerenciamento de Containers

```bash
# Parar todos os containers
docker compose down

# Reiniciar servi√ßos
docker compose restart

# Ver logs em tempo real
docker compose logs -f

# Limpar volumes (CUIDADO: apaga modelos baixados)
docker compose down -v
```
---

<a name="referencias"></a>
## ‚ùáÔ∏è Refer√™ncias

### Papers & Benchmarks

- **HumanEval**: [*Evaluating Large Language Models Trained on Code*](https://arxiv.org/abs/2107.03374) ‚Äî *Chen et al., 2021*
- **Product of Experts**: [*Training Products of Experts by Minimizing Contrastive Divergence*](https://www.cs.toronto.edu/~hinton/absps/nccd.pdf) ‚Äî *Hinton, 2002*
- **Efficient LLM Comparative Assessment**: [*A Product of Experts Framework for Pairwise Comparisons*](https://arxiv.org/abs/2404.06587) ‚Äî *Zhang et al., 2024* 
  
---

### Modelos Recomendados

| Modelo | Par√¢metros | Perfil de Uso |
|--------|------------|----------------|
| **CodeGemma** | 7B | Gera√ß√£o de c√≥digo eficiente e leve |
| **CodeLlama** | 7B | Modelo vers√°til para aplica√ß√µes gerais |
| **Qwen2.5-Coder** | 7B | Excelente equil√≠brio entre custo e desempenho |
| **StarCoder2** | 7B | Boa rela√ß√£o custo/qualidade para prototipagem |
| **DeepSeek-Coder** | 6.7B | Foco em m√°xima performance e precis√£o |

---

<a name="licenca"></a>
## ‚ùáÔ∏è Licen√ßa

Este projeto est√° licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

```
MIT License

Copyright (c) 2025 [Seu Nome]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

[...]
```

### Atribui√ß√µes

Este projeto utiliza:
- **HumanEval** ¬© OpenAI (MIT License)
- **Ollama** ¬© Ollama Inc (MIT License)
- **Radon** ¬© Michele Lacchia (MIT License)
- **Pylint** ¬© Python Code Quality Authority (GPL)

---

<a name="autores"></a>
## ‚ùáÔ∏è Autores

### Equipe de Desenvolvimento

- [Iza L. Ribeiro](https://github.com/Izalp)
- [Humberto G. F. Silva](https://github.com/humbertogfs55)
- [Caroliny A. Teixeira](https://github.com/carolinyat)


<a name="citacao-academica"></a>
## ‚ùáÔ∏è Cita√ß√£o Acad√™mica

Se voc√™ usar este framework em sua pesquisa, por favor cite:

```bibtex
@software{poe_evaluation2025,
  author = {Humberto and Iza and Caroliny},
  title = {PoE-Evaluation: Product of Experts Framework for LLM Code Evaluation},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/tcc-inatel-2025/poe-evaluation},
  version = {1.0.0}
}
```

<div align="center">

### ‚≠ê Se este projeto ajudou sua pesquisa ou trabalho, considere dar uma estrela!

**Desenvolvido com ‚ù§Ô∏è para a comunidade acad√™mica**

**Happy Evaluating! üéâ**

</div>
